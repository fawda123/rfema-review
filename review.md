## Package Review

*Please check off boxes as applicable, and elaborate in comments below.  Your review is not limited to these topics, as described in the reviewer guide*

- **Briefly describe any working relationship you have (had) with the package authors.**
- [x] As the reviewer I confirm that there are no [conflicts of interest](https://devguide.ropensci.org/policies.html#coi) for me to review this work (if you are unsure whether you are in conflict, please speak to your editor _before_ starting your review).

#### Documentation

The package includes all the following forms of documentation:

- [x] **A statement of need:** clearly stating problems the software is designed to solve and its target audience in README
  
  Very nice statement of need, but see my comments below.  I'm also wondering if you could narrow the target audience a bit.  Is this intended mostly for academics/researchers or more for general public interest (e.g., downloading flood maps, etc.)?  Clearing this up from the beginning could be helpful. 
  
- [x] **Installation instructions:** for the development version of package and any non-standard dependencies in README
- [x] **Vignette(s):** demonstrating major functionality that runs successfully locally]

  You have a very nice README file that explains some of the core rationale for the package, how to install, etc. I did not know this package had a vignette until the end of the README file, so consider moving that up to the beginning if users want to dive right into that to learn about the package. The README vignette link also points to an HTML file on GitHub, which is not a readable format. After I installed the package and I ran the following in my R console, it indicated no vignettes were found. I honestly don't know how to rectify, but this may not be a problem once it's hosted on ROpenSci.  Regardless, I could only access the vignette by forking the source repo and inspecting the .Rmd file.

  ```
  vignette(package = 'rfema')
  #> no vignettes found
  ```
  
- [x] **Function Documentation:** for all exported functions

  The description for `bulk_dl` is missing text, i.e., "For large files, the function"... does what? 
  
  Also, there are several functions in `helpers.R` that seem to be just that, used internally by the package in the core functions.  Do these need to be exported, i.e., would a user ever have a need to call these functions by themselves?
  
- [x] **Examples:** (that run successfully locally) for all exported functions
  
  Many of the examples in the help files return a lot of printed text in the console that is problematic for two reasons.  First, it complicates interpretation when first time users are trying to understand function output.  Second, this will not render well on the ROpenSci reference pages when the package is finally onboarded.  Consider how you can print the example output in a more user-friendly style.  For example, in the `fema_data_sets` example, you just call the function as is:
  
  ```
  fema_data_sets()
  ```
  
  Something like this would produce a cleaner output for the help files. 
  
  ```
  dat <- fema_data_sets()
  dim(dat)
  head(dat[, c(1:3)])
  ```
  
  Better yet, consider converting all output as `tibble` objects since there is already a nice print method that handles these issues.  I've made a more detailed comment about that below. 
  
- [ ] **Community guidelines:** including contribution guidelines in the README or CONTRIBUTING, and DESCRIPTION with `URL`, `BugReports` and `Maintainer` (which may be autogenerated via `Authors@R`).

  There should be links in the README to the CONTRIBUTING file. I'd also suggest including an issues template with `usethis::use_tidy_issue_template()`.

#### Functionality

- [x] **Installation:** Installation succeeds as documented.
- [x] **Functionality:** Any functional claims of the software been confirmed.
- [x] **Performance:** Any performance claims of the software been confirmed.

  I really appreciate you give the user an option of continuing API requests that may take some time (i.e., the `ask_before_call` argument in `open_fema`).  I would imagine users may also wonder how long a request will take before they make the decision and I think it would be useful to provide some approximation in the printed text on the console. I know this varies between platforms and ISPs, but is there some simple rule of thumb you could use to approximate request time, e.g., 1 second per 1000 records, then scale this up when you estimate number of requests? So instead of saying something like "it will take 500 individual API calls" you could also say "it will take approximately 8.33 minutes".  I don't know, maybe this is a ridiculous idea.  
 
- [x] **Automated tests:** Unit tests cover essential functions of the package and a reasonable range of inputs and conditions. All tests pass on the local machine.

  Tests did not pass locally, but I do note that the CI on GitHub Actions for the main repo has no problems.  I also was able to run the tests going through each test file in the tests/testthat folder by excluding the `vcr` functionality, so it's probably an issue on my end with the latter. I just point this out in case others had the same problem.
  
- [x] **Packaging guidelines**: The package conforms to the rOpenSci packaging guidelines.

Estimated hours spent reviewing: 5

- [x] Should the author(s) deem it appropriate, I agree to be acknowledged as a package reviewer ("rev" role) in the package DESCRIPTION file.

---

### Review Comments

Hi @dylan-turner25, thanks for the opportunity to review the `rfema` package.  I don't typically work with these kinds of data, so I've adopted the "[naive user](https://ropensci.org/blog/2017/08/22/first-package-review/)" approach where my first introduction to the package was to download and load it, run through some examples, and interpret the output.  My general comments relate to this experience of exploring the package for the first time with some preconceived idea of what I think it does.

Looking at the README, you make a nice case for why you developed the package. I particularly like that you showed a side-by-side comparison of how a query would work with and without `rfema`.  It's a good selling point, although it certainly applies to any R package that taps into an API.  Newer R users will find this appealing, but more seasoned users will probably wonder if there are other advantages to the package, or more importantly, are there any limitations using your package vs going the home-cooked `httr` route.  Any time you build a custom workflow (e.g., a set of functions to handle API requests), something else is often sacrificed (e.g., absolute control over API calls using `httr`).  Does your package offer any enhanced functionality that benefits users if they're sacrificing control over API requests with `httr`?  This is kind of an annoying question, but I ask it mostly because the convenience of making API requests may not be a selling point for all. 

For example, it looks like most of the function calls return date/time columns as characters, but consider having those columns formatted as POSIX objects to save the user time (maybe even formatted to the system timezone). This would be a nice added feature of your package that one would not get if building the API calls from scratch. One could readily plot time series data if the date/time columns were already formatted.     

The code for the example comparison that runs `open_fema` returns a nice printed output to the console showing iteration progress. However, some of the content doesn't make sense - what does the `FALSE` text mean?  Also, it may not be important to print all the text, just `1 of 3`, `2 of 3`, etc. and maybe only include `iterations completed:` at the top of the loop so as not to clutter the console output each time a new iteration of the loop is initiated.  

As an aside, I really like that the FEMA API does not require a key.  I know this is completely out of your hands, but getting connected to an API is often a critical hurdle that can prevent others from using what very well may be a very helpful package.  So, this is definitely a good-selling point because the package works out of the box without having to talk to a third-party.  Perhaps emphasize this a bit more in the README/vignette, i.e., most other APIs require a "complicated" token exchange process, etc.   

I think one of the major issues a naive user would have is understanding the datasets available from the API.  I really appreciate that you've included a function to view what's available (`fema_data_sets`), but there still is no easy way to understand what's included in each dataset aside from parsing the data frame (i.e., the `kable` calls truncate the description).  It seems the most accessible place to understand what's available is on the FEMA data sets web page [here](https://www.fema.gov/about/openfema/data-sets).  It would be good to include this link in the README file when you talk about the `fema_data_sets()` function (it's at the top of the README, but good to mention again), vignette, and help file for `fema_data_sets()` so users know where to look for a more accessible description.   

Also going through the vignette, it seems like one limitation of the package is that the data needs to be downloaded first to identify values that are queryable.  There's one point in the vignette where you state that the `parameter_values` function "returns the unique values of a variable contained in the first 1000 observations of a data set". One advantage of having an API is the ability to know these parameters before downloading the data, where the latter can often be quite large and downloading an entire dataset can be impractical.  Are there API requests that can be made to retrieve queryable paramaters, rather than having a function in the package that downloads the data first to identify these parameters?  

Another limitation is that you've hard-coded the download from which the queryable parameters are identified to the first 1000 rows, so this very likely does not represent the entire suite of parameters in a column of the entire dataset.  If the API only provides the ability to download the data, then there's no way around the issue, but you should state this in the vignette so users are aware of the limitation.  Personally, I would just download all of the data for a table I'm interested in, then query later in R (using `dplyr` for example).  If I've misunderstood how these functions work, then disregard. 

Finally, I really like the examples at the end of the vignette.  These gave me a good sense of how I could use this package to address some more general questions.  I suspect many users will find this section of the vignette helpful.  One minor concern about the examples is I would not use `kable` output to show results since this format is more for making easy to read HTML tables in a report-style document where the need to show source code is not important.  A user of the package would not use `kable` in their scripting workflow and instead would be interpreting output in the console directly.  Showing results in the README and vignette w/ `kable` makes the output less accessible from an exploratory data analysis workflow that may be more comfortable for many folks.  It also creates unnecessary code in your vignette that can distract from the more important code that shows how to use your functions. If the output from `open_fema` is formatted as a `tibble` (ideally inside the package functions), the print method will already take care of the table overflows in the rendered HTML. 

#### Coding issues 

Minor issue, but on the README file, please update the badge link for the RCMD checks to GitHub Actions.  It currently goes to a .svg file for the `ijtiff` package.  

Use this: 

`[![R-CMD-check](https://github.com/dylan-turner25/rfema/workflows/R-CMD-check/badge.svg)](https://github.com/dylan-turner25/rfema/actions)`

Instead of this:

`![R-CMD-check](https://github.com/ropensci/ijtiff/workflows/R-CMD-check/badge.svg)`

I've also noticed a few typos on the README, vignette, and function docs. Please make sure to proofread since I'm not sure the automated package checks have caught everything.  

Other than that, the package structure seems pretty solid.  The testing infrastructure on GitHub Actions is very comprehensive (covers multiple platforms and R versions) and I did not even notice a single ERROR, WARNING, or NOTE for any of the checks.  Code coverage at ~85% is pretty good too.  Nice work!

Cheers, 
Marcus
