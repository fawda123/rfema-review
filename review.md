## Package Review

*Please check off boxes as applicable, and elaborate in comments below.  Your review is not limited to these topics, as described in the reviewer guide*

- **Briefly describe any working relationship you have (had) with the package authors.**
- [ ] As the reviewer I confirm that there are no [conflicts of interest](https://devguide.ropensci.org/policies.html#coi) for me to review this work (if you are unsure whether you are in conflict, please speak to your editor _before_ starting your review).

#### Documentation

The package includes all the following forms of documentation:

- [x] **A statement of need:** clearly stating problems the software is designed to solve and its target audience in README
- [x] **Installation instructions:** for the development version of package and any non-standard dependencies in README
- [x] **Vignette(s):** demonstrating major functionality that runs successfully locally
- [ ] **Function Documentation:** for all exported functions
- [ ] **Examples:** (that run successfully locally) for all exported functions
- [ ] **Community guidelines:** including contribution guidelines in the README or CONTRIBUTING, and DESCRIPTION with `URL`, `BugReports` and `Maintainer` (which may be autogenerated via `Authors@R`).

#### Functionality

- [ ] **Installation:** Installation succeeds as documented.
- [ ] **Functionality:** Any functional claims of the software been confirmed.
- [ ] **Performance:** Any performance claims of the software been confirmed.
- [ ] **Automated tests:** Unit tests cover essential functions of the package and a reasonable range of inputs and conditions. All tests pass on the local machine.
- [ ] **Packaging guidelines**: The package conforms to the rOpenSci packaging guidelines.

Estimated hours spent reviewing:

- [ ] Should the author(s) deem it appropriate, I agree to be acknowledged as a package reviewer ("rev" role) in the package DESCRIPTION file.

---

### Review Comments

In addition to the general checklist above, my review adopted the "[naive user](https://ropensci.org/blog/2017/08/22/first-package-review/)" approach where my first introduction to the package was to download and load it, run through some examples, and interpret the output.  I did very little prior digging on the package, mostly because I think most users will engage with the package like they do with any other package. That is, they come across it by Googling to identify a potential solution to a problem they might have, they find this package, they install it, and try to get it to do what they think it will do. So, my first comments relate to this experience of exploring the package for the first time with some preconceived idea of what I think it does.  Some of the more detailed comments later speak to more specific components of the package structure. 

First off, you have a very nice README file that explains some of the core rationale for the package, how to install, etc. I did not know this package had a vignette until the end of the README file, so consider moving that up to the beginning if users want to dive right into that to learn about the package. It also links to an HTML file on GitHub, which is not a readable format. After I installed hte package and I ran the following in my R console, it indicated no vignettes were found. I honestly don't know how to rectify, but this may not be a problem once it's hosted on ROpenSci.  Regardless, I could only access the vignette by forking the source repo and inspecting the .Rmd file.

```
vignette(package = 'rfema')
#> no vignettes found
```

Next, I installed the package with devtools without any issue.  It also loaded from my library without a hitch (i.e, no weird warning message about R version, etc.).  

Looking at the README, you make a nice case for why you developed the package. I particularly like that you showed a side-by-side comparison of how a query would work with and without `rfema`.  It's a good selling point, although it certainly applies to any R package that taps into an API.  Newer R users will find this appealing, but more seasoned users will probably wonder if there are other advantages to the package, or more importantly, are there any limitations using your package vs going the conventional `httr` route.  Any time you build a custom workflow (e.g., a set of functions to handle API requests), something else is often sacrificed (e.g., absolute control over API calls using `httr`).  Does your package offer any enhanced functionality that benefits users if they're sacrificing control over API requests wiht `httr ?  This is kind of an annoying question, but I ask it mostly because the convenience of making API requests may not be a selling point for all. 

For example, it looks like most of the function calls return date/time columns as characters, but consider having those columns formatted as POSIX objects to save the user time. This would be a nice added feature of your package that one would not get if building the API calls from scratch. One could readily plot time series data if the date/time columns were already formatted.     

The code for the example comparison that runs `open_fema` returns a nice printed output the console showing iteration progress. However, some of the content doesn't make sense - what does the `FALSE` text mean?  Also, it may not be important to print all the text, just `1 of 3`, `2 of 3`, etc. and maybe only include `iterations completed:` at the top of the loop so as not to clutter the console output too much.  

As an aside, I really like that the FEMA API does not require a key.  I know this is completely out of your hands, but getting connected to an API is often a critical hurdle that can prevent others from using what very well may be a very useful package.  So, this is definitely a good-selling point for this package in that it works out of the box without having to talk to a third-party.    

I think one of the major issues a naive user would have is understanding the datasets available from the API.  I really appreciate that you've included a function to view what's available (`fema_data_sets()`), but there still is no easy way to understand what's included each dataset aside from parsing the data frame (i.e., the `kable` calls truncate the description).  It seems the most accessible place to understand what's available is on the FEMA data sets web page [here](https://www.fema.gov/about/openfema/data-sets).  It would be good to include this link in the README file when you talk about the `fema_data_sets()` function (it's at the top of the README, but good to mention again), vignette, and help file for `fema_data_sets()` so users know where to look for a more accessible description.   

Also going through the vignette, it seems like one limitation of the package is that the data needs to be downloaded first to identify values that are queryable.  There's one point in the vignette where you state that the `parameter_values` function "returns the unique values of a variable contained in the first 1000 observations of a data set". One advantage of having an API is the ability to know these parameters before downloading the data, where the latter can often be quite large and downloading an entire dataset can be impractical.  Are there API requests that can be made to retrieve queryable paramaters, rather than having a function in the package that downloads the data first to identify these parameters?  Another limitation of this approach is that you've hard-coded the download from which the queryable parameters are identified to the first 1000 rows, so this very likely does not represent the entire suite of parameters in a column.  If the API only provides the ability to downoad the data, then there's no way around the issue, but you should state this in the vignette so users are aware of the limitation.  Personally, I would just download all of the data for a table I'm interested in, then query later in R (using dplyr for example).  If I've misunderstood how these functions work, then disregard. 

Finally, I really like the examples at the end of the vignette.  These gave me a good sense of how I could use this package to address some more general questions.  I suspect many users will find this section of the vignette helpful.  One minor concern about the examples is I would not use `kable` output to show results since this format is more for making easy to read HTML tables in a report-style document where the need to show source code is not important.  A user of the package would not use `kable` in their scripting workflow and instead would be interpreting output in the console directly.  Showing results in the vignette w/ `kable` makes the output less accessible from an exploratory data analysis workflow that maybe more comfortable for many folks.  It also creates unnecessary code in your vignette that can distract from the more important code that shows how to use your functions. If the output from `open_fema` is formatted as a `tibble`, then the printed output the vignettes will already take care of the table overflows in the rendered HTML. 

#### Coding issues 

Minor issue, but on the README file, please update the badge link for the RCMD checks to GitHub Actions.  It currently goes to a .svg file for the `ijtiff` package.  

Use this: 

`[![R-CMD-check](https://github.com/dylan-turner25/rfema/workflows/R-CMD-check/badge.svg)](https://github.com/dylan-turner25/rfema/actions)`

Instead of this:

`![R-CMD-check](https://github.com/ropensci/ijtiff/workflows/R-CMD-check/badge.svg)`

I've also noticed a few typos on the README and VIGNETTE. Please make sure to proofread since I'm not sure the automated package checks have caught everything.  

I did not see any guidelines for contributing.  It's straightforward to add with `usethis::use_tidy_contributing()` (just edit out any references to the tidyverse and replace with rfema).  I'd also suggest including an issues template with `usethis::use_tidy_issue_template()`.

Other than that, the package structure seems pretty solid.  The testing infrastructure on GitHub Actions is very comprehensive (covers multiple platforms and R versions) and I did not even notice a single ERROR, WARNING, or NOTE for any of the checks.  Code coverage at ~85% is pretty good too.  Nice work!

